\section{Sketching}

Let $n=2741220$ and $d=21$, which are taken
from dimensions of data in Equation~\eqref{eq:data}.
Set $\varepsilon=0.1$ and $\delta=0.9$ in the following.
We gather all the testing results in Table~\ref{tab:grand}.

\begin{table}[htb]
  \setlength{\tabcolsep}{2.6pt}
  \caption{The performances of different Sketching techniques.
    other than the min loss, all are average values of repeats.
    \emph{prep} and \emph{app} are time spent on sketching and
    on solving the shrinked sized least square problem, respectively.
    ref-$k$ is the reference value for $k$.}
  \label{tab:grand}
  \centering
  {\small
  \begin{tabular}{lllllllllll}
    \toprule
    algorithm & repeat & ref-$k$ & $k$ & prep (s) & app (ms) & min loss & median loss & max loss & std loss & mean loss \\
    \midrule
    Baseline & 10 & - & - & 0 & 190 & 15090.7 & 15090.7 \\
    Gaussian & 100 & 2432 & 10 & 0.76 & 0.3 & 20479.8 & 76514.7 & 1268090 & 177191.0 & 126311 \\
    Gaussian & 100 & 2432 & 100 & 7.44 & 9.7 & 16080.5 & 16983.8 & 18631.3 & 583.3 & 17059.5 \\
    PHD & 100 & 37234 & 100 & 3.76 & 0.4 & 15927.6 & 16977.5 & 19309.7 & 635.3 & 16999.5 \\
    PHD & 100 & 37234 & 1000 & 3.72 & 1.1 & 15159.7 & 15252.5 & 15364.3 & 45.4 & 15248.1 \\
    Count & 100 & 340198 & 1000 & 1.17 & 0.4 & 15157.6 & 15240.8 & 15397.1 & 43.0 & 15246.7 \\
    Count & 100 & 340198 & 10000 & 1.20 & 1.1 & 15096.8 & 15105.3 & 15122.3 & 5.3 & 15106.5 \\
    Leverage & 100 \\
    Leverage & 100 \\
    \bottomrule
  \end{tabular}
  }
\end{table}

%
\subsection{Gaussian}
By Theorem~6 in the text book article~\cite{woodruff2014sketching},
\begin{equation}
    k=\Theta((d+\log(1/\delta))\varepsilon^{-2}),
\end{equation}
$k$ is a multiple of $(21+log(10))\times100\approx2432$.
This is too large for real computation, so we set $k=10$ and $k=100$.
Results are shown in Table~\ref{tab:grand}.
The minimum point when $k=100$ is
{
\def\OldComma{,}
\catcode`\,=13
\def,{%
	\ifmmode%
	\OldComma\discretionary{}{}{}%
	\else%
	\OldComma%
	\fi%
}%
$x^* = (0.595, 0.076, -0.849, 0.829, -0.142, -0.167, 0.909,
-0.788, 0.303, -16.9, 14.8, -51.4, 49.3, -0.637, 2.696, -1.721,
0.853, -0.084, 0.367, -0.393, -0.089)$ with loss $f(x^*) = 15781.0$.
}

%
\subsection{PHD}

$P$ is used for pick $k$ rows from matrix on the right hand side.
$H$ is Hardamard matrix and $D$ a diagonal matrix.
Since size of $H$ is required to be a power of 2,
we pad the the $n$ dimensional vectors to the
smallest power of 2 that is larger than $n$.
We use \emph{Hadamard.jl} package in Julia for the computation
of the Hadamard transform.
By Theorem~7, the selected row number
\begin{equation}
    k=\Omega\big((\log(d))(\sqrt{d}+\sqrt{\log(n)})^2\varepsilon^{-2}\big).
\end{equation}
A reference number for $k$ is
$\log(21)\times(\sqrt{21}+\sqrt{\log(2741220)})^2\times100\approx37234$.
This is too large for computation, thus we set $k=1000$ and $k=1000$.
Results are shown in Table~\ref{tab:grand}.
The minimum point when $k=1000$ is
{
\def\OldComma{,}
\catcode`\,=13
\def,{%
	\ifmmode%
	\OldComma\discretionary{}{}{}%
	\else%
	\OldComma%
	\fi%
}%
$x^* = ()$
with loss $f(x^*) = .0$.
}

%
\subsection{Count Sketch}
By Theorem~8, the $k$ (we use $k$ here for consistency,
$r$ is used in the article) is,
\begin{equation}
    k=\mathcal{O}\big(d^2\text{poly}(\log(d/\varepsilon))\varepsilon^{-2}\big).
\end{equation}
A reference number for $k$ is
$21^2\times\log(21/0.1)\times100\approx 340198$.
This is too large for computation, thus we set $k=1000$ and $k=10000$.
Results are shown in Table~\ref{tab:grand}.
The minimum point when $k=10000$ is
{
\def\OldComma{,}
\catcode`\,=13
\def,{%
	\ifmmode%
	\OldComma\discretionary{}{}{}%
	\else%
	\OldComma%
	\fi%
}%
$x^* = ()$
with loss $f(x^*) = .0$.
}

\subsection{Leverage Score}
