\section{Sketching}

Let $n=2741220$ and $d=21$, which are taken
from dimensions of data in Equation~\eqref{eq:data}.
Set $\varepsilon=0.1$ and $\delta=0.9$ in the following.
We gather all the testing results in Table~\ref{tab:grand}.

\begin{table}[htb]
  \setlength{\tabcolsep}{2.6pt}
  \caption{The performances of different Sketching techniques.
    other than the min loss, all are average values of repeats.
    \emph{prep} and \emph{app} are time spent on sketching and
    on solving the shrinked sized least square problem, respectively.
    ref-$k$ is the reference value for $k$.}
  \label{tab:grand}
  \centering
  {\small
  \begin{tabular}{lllllllllll}
    \toprule
    algorithm & repeat & ref-$k$ & $k$ & prep (s) & app (ms) & min loss & median loss & max loss & std loss & mean loss \\
    \midrule
    Baseline & 10 & - & - & 0 & 190 & 15090.7 & 15090.7 & 15090.7 & 0 & 15090.7 \\
    Gaussian & 100 & 2432 & 10 & 0.76 & 0.3 & 20479.8 & 76514.7 & 1268090 & 177191.0 & 126311 \\
    Gaussian & 100 & 2432 & 100 & 7.44 & 9.7 & 16080.5 & 16983.8 & 18631.3 & 583.3 & 17059.5 \\
    PHD & 100 & 37234 & 100 & 3.76 & 0.4 & 15927.6 & 16977.5 & 19309.7 & 635.3 & 16999.5 \\
    PHD & 100 & 37234 & 1000 & 3.72 & 1.1 & 15159.7 & 15252.5 & 15364.3 & 45.4 & 15248.1 \\
    Count & 100 & 340198 & 1000 & 1.17 & 0.4 & 15157.6 & 15240.8 & 15397.1 & 43.0 & 15246.7 \\
    Count & 100 & 340198 & 10000 & 1.20 & 1.1 & 15096.8 & 15105.3 & 15122.3 & 5.3 & 15106.5 \\
    Leverage & 100 & 1826573 & 10000 & 1.15 & 1.1 & 15098.8 & 15106.4 & 15119.0 & 4.7 & 15106.8 \\
    Leverage & 100 & 1826573 & 100000 & 1.70 & 22.9 & 15091.3 & 15092.2 & 15094.2 & 0.5 & 15092.3 \\
    \bottomrule
  \end{tabular}
  }
\end{table}

%
\subsection{Gaussian}
By Theorem~6 in the text book article~\cite{woodruff2014sketching},
\begin{equation}
    k=\Theta((d+\log(1/\delta))\varepsilon^{-2}),
\end{equation}
$k$ is a multiple of $(21+log(10))\times100\approx2432$.
This is too large for real computation, so we set $k=10$ and $k=100$.
Results are shown in Table~\ref{tab:grand}.
The minimum point when $k=100$ is
{
\def\OldComma{,}
\catcode`\,=13
\def,{%
	\ifmmode%
	\OldComma\discretionary{}{}{}%
	\else%
	\OldComma%
	\fi%
}%
$x^* = (0.292,0.162,0.83,-1.478,0.693,0.034,-0.851,1.482,-0.614,1.286,-29.447,-12.258,37.858,0.534,5.043,-5.779,-0.274,-0.157,-0.212,0.753,-0.234)$
with loss $f(x^*) = 16080.5$.
}

%
\subsection{PHD}

$P$ is used for pick $k$ rows from matrix on the right hand side.
$H$ is Hardamard matrix and $D$ a diagonal matrix.
Since size of $H$ is required to be a power of 2,
we pad the the $n$ dimensional vectors to the
smallest power of 2 that is larger than $n$.
We use \emph{Hadamard.jl} package in Julia for the computation
of the Hadamard transform.
By Theorem~7, the selected row number
\begin{equation}
    k=\Omega\big((\log(d))(\sqrt{d}+\sqrt{\log(n)})^2\varepsilon^{-2}\big).
\end{equation}
A reference number for $k$ is
$\log(21)\times(\sqrt{21}+\sqrt{\log(2741220)})^2\times100\approx37234$.
This is too large for computation, thus we set $k=1000$ and $k=1000$.
Results are shown in Table~\ref{tab:grand}.
The minimum point when $k=1000$ is
{
\def\OldComma{,}
\catcode`\,=13
\def,{%
	\ifmmode%
	\OldComma\discretionary{}{}{}%
	\else%
	\OldComma%
	\fi%
}%
$x^* = (0.221,0.002,0.021,0.193,-0.034,-0.048,0.027,-0.168,0.24,-1.341,-4.14,-9.779,10.611,-0.129,-0.359,1.604,-0.894,-0.035,-0.121,0.28,-0.065)$
with loss $f(x^*) = 15159.7$.
}

%
\subsection{Count Sketch}
By Theorem~8, the $k$ (we use $k$ here for consistency,
$r$ is used in the article) is,
\begin{equation}
    k=\mathcal{O}\big(d^2\text{poly}(\log(d/\varepsilon))\varepsilon^{-2}\big).
\end{equation}
A reference number for $k$ is
$21^2\times\log(21/0.1)\times100\approx 340198$.
This is too large for computation, thus we set $k=1000$ and $k=10000$.
Results are shown in Table~\ref{tab:grand}.
The minimum point when $k=10000$ is
{
\def\OldComma{,}
\catcode`\,=13
\def,{%
	\ifmmode%
	\OldComma\discretionary{}{}{}%
	\else%
	\OldComma%
	\fi%
}%
$x^* = (0.179,-0.007,0.004,0.036,0.09,0.007,-0.09,0.083,0.081,-0.871,-1.128,4.666,-6.164,0.127,0.214,-0.183,-0.245,0.011,0.004,0.02,0.002)$
with loss $f(x^*) = 15096.8$.
}

\subsection{Leverage Score}
We use the procedure described by Definition~16.
For simplicity, we did not implement the fancier one
as in Theorem~19.
In this case, $q$ is selected as $p$
and $\beta=1$.
By Theorem~17, the $k$ (we use $k$ here for consistency,
$s$ is used in the article,
and the $k$ of the article correspond to $d$ here) is,
\begin{equation}
    k>144d\ln(2d/\delta)\beta^{-1}\varepsilon^{-2}.
\end{equation}
A reference number for $k$ is
$144\times 21 \times \ln(2\times 21 \times 10)\times100\approx 1826573$.
This is too large for computation, thus we set $k=10000$ and $k=100000$.
Results are shown in Table~\ref{tab:grand}.
The minimum point when $k=10000$ is
{
\def\OldComma{,}
\catcode`\,=13
\def,{%
	\ifmmode%
	\OldComma\discretionary{}{}{}%
	\else%
	\OldComma%
	\fi%
}%
$x^* = (0.19,-0.008,-0.03,0.045,0.112,-0.026,-0.044,0.057,0.088,1.016,0.542,-5.59,0.536,0.012,0.037,0.111,-0.124,-0.001,0.035,0.031,-0.047)$
with loss $f(x^*) = 15091.3$.
}
This is the best result in our experiment.
Especially, notice that both $x^*$ and $f(x^*)$ are close to the ground truth.
