\section{Discussion}

From the experiments,
we can observe that
\begin{itemize}
    \item In the Frobenius norm setting,
        most of the time was spent on the second phase,
        \ie solving the smaller sized problem.
        Specifically,
        Almost all time was used in the multiplication $AQ$.
    \item To accelerate this multiplication,
        we tried multiple methods.
        The interesting fact is that,
        by converting matrix $A$ from sparse to full,
        the multiplication will be much faster
        (50s v.s. 5s).
        This might due to cache or memory alignment or so.
        It would be good if sketching
        that keep operator norm of the product of two matrices
        can be used here.
    \item The error of sketching in different runs is stable
        as we can see from the small std values.
        The relative error is below 5\% when sketching to
        $t=1024$ dimension.
        Smaller $t$ leads larger error.
        Due to to time consuming multiplication,
        the time gain is not ideal.
        In the $t=1024$ case,
        we only achieved a 2x speed up.
    \item The original algorithm low-rank approximation under operator norm
        has some numerical issues as we discussed in the corresponding section.
        We solved the problem by inserting an additional normalize operation.
    \item If we want to get high precision using subspace power method
        in the operator norm case, the running time may be not favorable
        as we have seen in the last row of Table~\ref{tab:fix}.
        This is because the method contains lots of matrix multiplications,
        which are time consuming.
    \item In both of the cases,
        sketching method for matrix multiplication that keep operator norm
        is wanted since the speed bottleneck lies in
        the multiplication operation.
\end{itemize}
